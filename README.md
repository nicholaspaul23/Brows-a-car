This program was created to simulate shopping for cars online by only using your facial expressions and parsing them to collect data on the items you like. This data is then processed to find the best categories of cars to show the user next. This program uses computer vision methods such as facial detection and facial recognition to detect and recognize users logged in. It implements affecting computing by analyzing the facial expressions of a user and gathering emotional responses to car advertisements shown. I built and trained a facial expression recognition neural network (with image augmentation) to classify users' facial expressions into seven categories that could detect anger, disgust, fear, happiness, sadness, surprise, and neutral expressions. An algorithm gathers and processes the responses to the cars shown in real time over various frames of video capture. Responses are recorded with a weighted average calculated to get the overall emotion for the given car. Using a large dataset of cars (and car info such as MSRP), unsupervised machine learning was used to gain insight into how each car is related to each other. A DBSCAN model generates clusters that group similar cars together. These car categories are used to recommend other cars the user may like based on their responses to the shown car advertisement images. Lastly, once all the results are tallied, the category of cars the user liked the most is determined and other cars in the same category are suggested.

https://user-images.githubusercontent.com/47327154/193425230-81f9f48d-94a6-498a-84a3-0f9bd73554ce.mp4

